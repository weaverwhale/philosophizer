# Philosophizer - Docker Compose Configuration
#
# Usage:
#   docker compose up -d           # Start all services
#   docker compose up -d --build   # Rebuild and start
#   docker compose down            # Stop all services
#   docker compose logs -f         # View logs
#
# First time setup:
#   1. Create .env file with required variables (see .env.example)
#   2. docker compose up -d
#   3. Wait for ollama to download the model (check logs: docker compose logs -f ollama)
#   4. PostgreSQL schema will be automatically initialized
#   5. Index philosopher texts: docker compose exec app bun run rag:index

services:
  # ==========================================================================
  # PostgreSQL Database with pgvector
  # ==========================================================================
  # Use base image for local development, or use pre-loaded image for production:
  #   image: mjweaver01/philosophizer-pgvector:latest
  postgres:
    image: pgvector/pgvector:pg16
    container_name: philosophizer-postgres
    restart: unless-stopped
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-philosophizer}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./src/db/schema.sql:/docker-entrypoint-initdb.d/schema.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ==========================================================================
  # Ollama - Local LLM & Embedding Server
  # ==========================================================================
  # Commented out - using LM Studio instead
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: philosophizer-ollama
  #   restart: unless-stopped
  #   ports:
  #     - "${OLLAMA_PORT:-11434}:11434"
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   healthcheck:
  #     test: ["CMD", "ollama", "list"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 10s

  # Pull the nomic-embed-text model on startup
  # ollama-pull-embeddings:
  #   image: ollama/ollama:latest
  #   container_name: philosophizer-ollama-pull-embeddings
  #   depends_on:
  #     ollama:
  #       condition: service_healthy
  #   entrypoint: ["ollama", "pull", "nomic-embed-text"]
  #   environment:
  #     - OLLAMA_HOST=http://ollama:11434
  #   restart: "no"

  # Pull the Qwen3 1.7B model for chat
  # ollama-pull-qwen:
  #   image: ollama/ollama:latest
  #   container_name: philosophizer-ollama-pull-qwen
  #   depends_on:
  #     ollama:
  #       condition: service_healthy
  #   entrypoint: ["ollama", "pull", "qwen3:1.7b"]
  #   environment:
  #     - OLLAMA_HOST=http://ollama:11434
  #   restart: "no"

  # ==========================================================================
  # Philosophizer Application
  # ==========================================================================
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: philosophizer-app
    restart: unless-stopped
    ports:
      - "${PORT:-1738}:1738"
    environment:
      - NODE_ENV=production
      - PORT=1738
      # Database connection (internal Docker network)
      - DATABASE_URL=postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-philosophizer}
      # JWT Configuration
      - JWT_SECRET=${JWT_SECRET:-change-this-in-production}
      - JWT_EXPIRES_IN=${JWT_EXPIRES_IN:-7d}
      # LLM (chat) provider configuration - using LM Studio on host machine
      - AI_BASE_URL=${AI_BASE_URL:-http://host.docker.internal:1234/v1}
      - LLM_MODEL=${LLM_MODEL}
      # Accept either AI_API_KEY or OPENAI_API_KEY (not needed for LM Studio, but kept for compatibility)
      - AI_API_KEY=${AI_API_KEY:-lm-studio}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - SEARCH_MODEL=${SEARCH_MODEL}
      # Embedding service - using LM Studio on host machine
      - EMBEDDING_BASE_URL=${EMBEDDING_BASE_URL:-http://host.docker.internal:1234/v1}
      - EMBEDDING_API_KEY=${EMBEDDING_API_KEY:-lm-studio}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
    depends_on:
      postgres:
        condition: service_healthy
      # Removed ollama dependencies - using LM Studio on host instead
    extra_hosts:
      # Allow container to access host machine services (for local LM Studio)
      - "host.docker.internal:host-gateway"

# ==========================================================================
# Persistent Volumes
# ==========================================================================
volumes:
  postgres-data:
    name: philosophizer-postgres-data
  # Commented out - not needed when using LM Studio
  # ollama-data:
  #   name: philosophizer-ollama-data
